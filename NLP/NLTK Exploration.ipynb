{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!python -m nltk.downloader twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!python -m nltk.downloader averaged_perceptron_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative_tweets.json', 'positive_tweets.json']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = twitter_samples.strings('positive_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_tweets_tokens = word_tokenize(tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " 'FollowFriday',\n",
       " '@',\n",
       " 'France_Inte',\n",
       " '@',\n",
       " 'PKuchly57',\n",
       " '@',\n",
       " 'Milipol_Paris',\n",
       " 'for',\n",
       " 'being',\n",
       " 'top',\n",
       " 'engaged',\n",
       " 'members',\n",
       " 'in',\n",
       " 'my',\n",
       " 'community',\n",
       " 'this',\n",
       " 'week',\n",
       " ':',\n",
       " ')']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tweets_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_tokens = twitter_samples.tokenized('positive_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#FollowFriday',\n",
       " '@France_Inte',\n",
       " '@PKuchly57',\n",
       " '@Milipol_Paris',\n",
       " 'for',\n",
       " 'being',\n",
       " 'top',\n",
       " 'engaged',\n",
       " 'members',\n",
       " 'in',\n",
       " 'my',\n",
       " 'community',\n",
       " 'this',\n",
       " 'week',\n",
       " ':)']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_tagged = pos_tag_sents(tweets_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#FollowFriday', 'JJ'),\n",
       " ('@France_Inte', 'NNP'),\n",
       " ('@PKuchly57', 'NNP'),\n",
       " ('@Milipol_Paris', 'NNP'),\n",
       " ('for', 'IN'),\n",
       " ('being', 'VBG'),\n",
       " ('top', 'JJ'),\n",
       " ('engaged', 'VBN'),\n",
       " ('members', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('community', 'NN'),\n",
       " ('this', 'DT'),\n",
       " ('week', 'NN'),\n",
       " (':)', 'NN')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tweets_tagged[tweet][word][pos]\n",
    "tweets_tagged[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Thai/NNP)\n",
      "  and/CC\n",
      "  (PERSON Sarah/NNP)\n",
      "  got/VBD\n",
      "  married/VBN\n",
      "  over/IN\n",
      "  the/DT\n",
      "  weekend/NN\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (FACILITY Wyndham/NNP Hotel/NNP)\n",
      "  ./.\n",
      "  It/PRP\n",
      "  was/VBD\n",
      "  a/DT\n",
      "  lovely/JJ\n",
      "  ceremony/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize, word_tokenize, pos_tag, ne_chunk\n",
    " \n",
    "sentence = \"Thai and Sarah got married over the weekend at the Wyndham Hotel. It was a lovely ceremony.\"\n",
    "print(ne_chunk(pos_tag(word_tokenize(sentence))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Thai/NNP)\n",
      "  and/CC\n",
      "  (PERSON Sarah/NNP)\n",
      "  got/VBD\n",
      "  married/VBN\n",
      "  over/IN\n",
      "  the/DT\n",
      "  weekend/NN\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (FACILITY Wyndham/NNP Hotel/NNP)\n",
      "  ./.\n",
      "  It/PRP\n",
      "  was/VBD\n",
      "  a/DT\n",
      "  lovely/JJ\n",
      "  ceremony/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "\n",
    "def iob_tag(sentance):\n",
    "    ne_tree = ne_chunk(pos_tag(word_tokenize(sentance))) \n",
    "    iob_tagged = tree2conlltags(ne_tree)\n",
    "    return iob_tagged\n",
    "\n",
    "ne_tree = conlltags2tree(iob_tagged)\n",
    "print(ne_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentance = \"Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems. Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iob_sentance = iob_tag(sentance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entities = []\n",
    "for ne_chunk in iob_sentance:\n",
    "    if ne_chunk[2] != 'O':\n",
    "        entities.append(ne_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Machine', 'NN', 'B-GPE'),\n",
       " ('Machine', 'NNP', 'B-PERSON'),\n",
       " ('Silicon', 'NNP', 'B-PERSON'),\n",
       " ('Valley', 'NNP', 'I-PERSON'),\n",
       " ('AI', 'NNP', 'B-ORGANIZATION')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import GMB NE wordbank\n",
    "source: http://gmb.let.rug.nl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uniq_ner_tags = [key.split('-')[0] for key in ner_tags.keys() if key != 'O']\n",
    "uniq_ner_tags = [tag for tag in set(uniq_ner_tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ner_dict = {}\n",
    "for tag in uniq_ner_tags:\n",
    "    ner_dict[tag] = collections.Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'O': 1146068, 'geo': 58388, 'org': 48094, 'per': 44254, 'tim': 34789, 'gpe': 20680, 'art': 867, 'eve': 709, 'nat': 300})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import collections\n",
    " \n",
    "ner_tags = collections.Counter()\n",
    " \n",
    "corpus_root = '../Downloads/gmb-2.2.0/'   # Make sure you set the proper path to the unzipped corpus\n",
    " \n",
    "for root, dirs, files in os.walk(corpus_root):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".tags\"):\n",
    "            with open(os.path.join(root, filename), 'rb') as file_handle:\n",
    "                file_content = file_handle.read().decode('utf-8').strip()\n",
    "                annotated_sentences = file_content.split('\\n\\n')   # Split sentences\n",
    "                for annotated_sentence in annotated_sentences:\n",
    "                    annotated_tokens = [seq for seq in annotated_sentence.split('\\n') if seq]  # Split words\n",
    "                    \n",
    "                    for idx, annotated_token in enumerate(annotated_tokens):\n",
    "                        annotations = annotated_token.split('\\t')   # Split annotations\n",
    "                        word, tag, ner = annotations[0], annotations[1], annotations[3]\n",
    "                        \n",
    "                        if ner != 'O':\n",
    "                            ner = ner.split('-')[0]\n",
    "                        ner_tags[ner] += 1\n",
    "                        \n",
    "                        if ner != 'O':\n",
    "                            #ner_dict[ner].add(word)\n",
    "                            ner_dict[ner][word] += 1\n",
    "\n",
    "print(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Bush': 1142,\n",
       "         'President': 3174,\n",
       "         'Mahmoud': 291,\n",
       "         'Ahmadinejad': 146,\n",
       "         'Thomas': 39,\n",
       "         'Horbach': 1,\n",
       "         'Abdullahi': 15,\n",
       "         'Yusuf': 22,\n",
       "         'Ahmad': 38,\n",
       "         'Prophet': 12,\n",
       "         'Muhammad': 19,\n",
       "         'Omar': 85,\n",
       "         'Khayam': 3,\n",
       "         'Malik': 12,\n",
       "         'Faridullah': 1,\n",
       "         'Khan': 77,\n",
       "         'Abdul': 71,\n",
       "         'Qadeer': 7,\n",
       "         'Nancy-Amelia': 1,\n",
       "         'Collins': 5,\n",
       "         'Tim': 14,\n",
       "         'Harcourt': 5,\n",
       "         'Foreign': 25,\n",
       "         'Minister': 860,\n",
       "         'Mustafa': 16,\n",
       "         'Osman': 20,\n",
       "         'Ismail': 36,\n",
       "         'Mr.': 4000,\n",
       "         'Cholily': 1,\n",
       "         'Azahari': 8,\n",
       "         'bin': 118,\n",
       "         'Husin': 8,\n",
       "         'Pope': 53,\n",
       "         'Benedict': 83,\n",
       "         'Sister': 1,\n",
       "         'Leonella': 1,\n",
       "         'Sgorbati': 1,\n",
       "         'Byzantine': 1,\n",
       "         'Muhammed': 4,\n",
       "         'Ahmed': 102,\n",
       "         'Qureia': 27,\n",
       "         'Prime': 998,\n",
       "         'Ehud': 59,\n",
       "         'Olmert': 86,\n",
       "         'Abbas': 304,\n",
       "         'George': 103,\n",
       "         'Major': 4,\n",
       "         'General': 215,\n",
       "         'Udi': 1,\n",
       "         'Adam': 18,\n",
       "         'Lieutenant': 4,\n",
       "         'Dan': 15,\n",
       "         'Halutz': 6,\n",
       "         'Tasnim': 5,\n",
       "         'Aslam': 7,\n",
       "         'June': 1,\n",
       "         'Soh': 1,\n",
       "         'Amy': 2,\n",
       "         'Katz': 1,\n",
       "         'Fidel': 75,\n",
       "         'Castro': 161,\n",
       "         'Martin': 35,\n",
       "         'Torrijos': 4,\n",
       "         'Vice': 84,\n",
       "         'Carlos': 40,\n",
       "         'Lage': 7,\n",
       "         'Mireya': 2,\n",
       "         'Moscoso': 2,\n",
       "         'Haniyeh': 26,\n",
       "         'Felipe': 30,\n",
       "         'Calderon': 28,\n",
       "         'Nicole': 4,\n",
       "         'Ritchie': 4,\n",
       "         'Dianne': 1,\n",
       "         'Sawyer': 1,\n",
       "         'Joel': 4,\n",
       "         'Madden': 2,\n",
       "         'Senator': 114,\n",
       "         'John': 263,\n",
       "         'Warner': 2,\n",
       "         'Deputy': 6,\n",
       "         'Interior': 5,\n",
       "         'Sutham': 1,\n",
       "         'Saengprathum': 1,\n",
       "         'Thaksin': 28,\n",
       "         'Shinawatra': 19,\n",
       "         'State': 14,\n",
       "         'Councilor': 1,\n",
       "         'Tang': 1,\n",
       "         'Jiaxuan': 1,\n",
       "         'Nobutaka': 3,\n",
       "         'Machimura': 4,\n",
       "         'Li': 18,\n",
       "         'Zhaoxing': 3,\n",
       "         'Tony': 101,\n",
       "         'Blair': 148,\n",
       "         'Sam': 12,\n",
       "         'Beattie': 5,\n",
       "         'Dharmeratnam': 3,\n",
       "         'Sivaram': 7,\n",
       "         'Saddam': 181,\n",
       "         'Hussein': 137,\n",
       "         'Chairman': 3,\n",
       "         'Paul': 158,\n",
       "         'Volcker': 8,\n",
       "         'Tynychbek': 1,\n",
       "         'Akmatbayev': 2,\n",
       "         'Murat': 4,\n",
       "         'Sutalinov': 1,\n",
       "         'Trade': 3,\n",
       "         'Anne-Marie': 1,\n",
       "         'Idrac': 1,\n",
       "         'Hugo': 188,\n",
       "         'Chavez': 454,\n",
       "         'Alvaro': 38,\n",
       "         'Uribe': 50,\n",
       "         'Defense': 5,\n",
       "         'Juan': 20,\n",
       "         'Manuel': 30,\n",
       "         'Santos': 7,\n",
       "         'Liu': 12,\n",
       "         'Jianchao': 1,\n",
       "         'Joseph': 39,\n",
       "         'Kony': 13,\n",
       "         'Roger': 16,\n",
       "         'Federer': 13,\n",
       "         'Tommy': 6,\n",
       "         'Haas': 5,\n",
       "         'Kizza': 4,\n",
       "         'Besigye': 17,\n",
       "         'Dr.': 31,\n",
       "         'Yoweri': 14,\n",
       "         'Museveni': 24,\n",
       "         'Bosco': 1,\n",
       "         'Katutsi': 1,\n",
       "         'Barack': 116,\n",
       "         'Obama': 320,\n",
       "         'Regina': 1,\n",
       "         'Benjamin': 28,\n",
       "         'Surgeon': 1,\n",
       "         'Wolfowitz': 11,\n",
       "         'Ariel': 101,\n",
       "         'Sharon': 214,\n",
       "         'Susilo': 3,\n",
       "         'Bambang': 12,\n",
       "         'Yudhoyono': 13,\n",
       "         'Rachid': 1,\n",
       "         'Ramda': 5,\n",
       "         'Yu': 4,\n",
       "         'Hagino': 1,\n",
       "         'Bakri': 5,\n",
       "         'Mohammed': 110,\n",
       "         'Osama': 87,\n",
       "         'Laden': 105,\n",
       "         'Mike': 19,\n",
       "         'Mullen': 5,\n",
       "         'Admiral': 2,\n",
       "         'Brigadier': 4,\n",
       "         'Vahidi': 2,\n",
       "         'Mikhail': 43,\n",
       "         'Saakashvili': 37,\n",
       "         'Alberto': 27,\n",
       "         'Fujimori': 38,\n",
       "         'Abdullah': 99,\n",
       "         'Badawi': 2,\n",
       "         'Wael': 1,\n",
       "         'al-Rubaei': 1,\n",
       "         'Levy': 7,\n",
       "         'Mwanawasa': 12,\n",
       "         'Information': 4,\n",
       "         'Mulongoti': 1,\n",
       "         'Robert': 93,\n",
       "         'Mugabe': 47,\n",
       "         'Gene': 1,\n",
       "         'Sperling': 4,\n",
       "         'Bill': 72,\n",
       "         'Clinton': 162,\n",
       "         'Lawrence': 5,\n",
       "         'Summers': 1,\n",
       "         'Treasury': 1,\n",
       "         'Secretary': 136,\n",
       "         'Timothy': 7,\n",
       "         'Geithner': 2,\n",
       "         'William': 28,\n",
       "         'Daley': 1,\n",
       "         'Rahm': 1,\n",
       "         'Emmanuel': 5,\n",
       "         'Dragan': 8,\n",
       "         'Neskovic': 1,\n",
       "         'Radovan': 8,\n",
       "         'Karadzic': 11,\n",
       "         'Philip': 6,\n",
       "         'Alston': 5,\n",
       "         'King': 105,\n",
       "         'Bhumibol': 1,\n",
       "         'Adulyadej': 1,\n",
       "         'Ghormley': 2,\n",
       "         'Meles': 10,\n",
       "         'Zenawi': 11,\n",
       "         'James': 53,\n",
       "         'Obita': 1,\n",
       "         'Abu': 88,\n",
       "         'al-Baghdadi': 4,\n",
       "         'Kevin': 21,\n",
       "         'Bergner': 1,\n",
       "         'Hemingway': 1,\n",
       "         'Adnan': 9,\n",
       "         'al-Dulaymi': 2,\n",
       "         'Dulaymi': 1,\n",
       "         'Moqtada': 26,\n",
       "         'al-Sadr': 29,\n",
       "         'Ron': 7,\n",
       "         'Redmond': 2,\n",
       "         'Hashem': 1,\n",
       "         'Ashibli': 1,\n",
       "         'Saadoun': 3,\n",
       "         'al-Dulaimi': 14,\n",
       "         'Ibrahim': 63,\n",
       "         'Bahr': 3,\n",
       "         'al-Uloum': 3,\n",
       "         'Zoba': 1,\n",
       "         'Yass': 1,\n",
       "         'Raza': 18,\n",
       "         'Gilani': 20,\n",
       "         'Luis': 44,\n",
       "         'Posada': 10,\n",
       "         'Carriles': 9,\n",
       "         'Pervez': 104,\n",
       "         'Musharraf': 159,\n",
       "         'Saud': 9,\n",
       "         'Aziz': 61,\n",
       "         'Agha': 4,\n",
       "         'Ziauddin': 1,\n",
       "         'MSWATI': 1,\n",
       "         'III': 4,\n",
       "         'Fernando': 8,\n",
       "         'Andreu': 1,\n",
       "         'Health': 1,\n",
       "         'Gao': 2,\n",
       "         'Qiang': 3,\n",
       "         'Jens': 1,\n",
       "         'Stoltenberg': 1,\n",
       "         'Wangari': 1,\n",
       "         'Maathai': 1,\n",
       "         'al-Zarqawi': 77,\n",
       "         'Hamid': 148,\n",
       "         'Karzai': 189,\n",
       "         'Ivanov': 32,\n",
       "         'Gilchrist': 2,\n",
       "         'Silvio': 12,\n",
       "         'Berlusconi': 25,\n",
       "         'Romano': 4,\n",
       "         'Prodi': 11,\n",
       "         'Moses': 2,\n",
       "         'Bittok': 3,\n",
       "         'Shinzo': 6,\n",
       "         'Abe': 10,\n",
       "         'Stephen': 21,\n",
       "         'Hadley': 9,\n",
       "         'Obeidi': 1,\n",
       "         'Markos': 1,\n",
       "         'Kypriano': 1,\n",
       "         'Roche': 15,\n",
       "         'Margaret': 17,\n",
       "         'Chan': 8,\n",
       "         'Yury': 3,\n",
       "         'Yekhanurov': 13,\n",
       "         'Daw': 1,\n",
       "         'Sint': 4,\n",
       "         'Aung': 39,\n",
       "         'HMS': 1,\n",
       "         'Scott': 36,\n",
       "         'Bob': 15,\n",
       "         'Ney': 3,\n",
       "         'Jack': 31,\n",
       "         'Abramoff': 11,\n",
       "         'Tom': 27,\n",
       "         'DeLay': 16,\n",
       "         'Hamdi': 8,\n",
       "         'Issac': 7,\n",
       "         'Hussain': 12,\n",
       "         'Augusto': 14,\n",
       "         'Pinochet': 40,\n",
       "         'Victor': 12,\n",
       "         'Montiglio': 2,\n",
       "         'Operation': 1,\n",
       "         'Colombo': 2,\n",
       "         'Contreras': 1,\n",
       "         'Evo': 25,\n",
       "         'Morales': 57,\n",
       "         'Luiz': 26,\n",
       "         'Inacio': 27,\n",
       "         'Lula': 27,\n",
       "         'da': 38,\n",
       "         'Silva': 46,\n",
       "         'Nicanor': 2,\n",
       "         'Duarte': 1,\n",
       "         'Kerry': 14,\n",
       "         'Ken': 4,\n",
       "         'Salazar': 3,\n",
       "         'Somalians': 1,\n",
       "         'Farid': 6,\n",
       "         'Balad': 1,\n",
       "         'Ruz': 1,\n",
       "         'Ali': 168,\n",
       "         'al-Nuimei': 1,\n",
       "         'Sisco': 21,\n",
       "         'Donald': 69,\n",
       "         'Rumsfeld': 106,\n",
       "         'Musab': 34,\n",
       "         'Iyad': 31,\n",
       "         'Allawi': 52,\n",
       "         'Shaowu': 1,\n",
       "         'Carol': 22,\n",
       "         'Pearson': 21,\n",
       "         'Costello': 2,\n",
       "         'Hassan': 69,\n",
       "         'Hushiar': 1,\n",
       "         'Condoleezza': 138,\n",
       "         'Rice': 278,\n",
       "         'Jalal': 49,\n",
       "         'Talabani': 69,\n",
       "         'Ms.': 237,\n",
       "         'Dick': 45,\n",
       "         'Cheney': 89,\n",
       "         'Adil': 3,\n",
       "         'Mahdi': 4,\n",
       "         'Saeb': 9,\n",
       "         'Erekat': 9,\n",
       "         'Nouri': 54,\n",
       "         'al-Maliki': 56,\n",
       "         'Mstislav': 1,\n",
       "         'Rostropovich': 3,\n",
       "         'Vladimir': 119,\n",
       "         'Putin': 179,\n",
       "         'Galina': 2,\n",
       "         'Vishnevskaya': 1,\n",
       "         'Andre': 8,\n",
       "         'Agassi': 14,\n",
       "         'Guillermo': 5,\n",
       "         'Garcia-Lopez': 2,\n",
       "         'Xavier': 2,\n",
       "         'Malisse': 3,\n",
       "         'Florian': 3,\n",
       "         'Mayer': 6,\n",
       "         'Gilles': 2,\n",
       "         'Muller': 2,\n",
       "         'Vince': 2,\n",
       "         'Spadea': 2,\n",
       "         'Lee': 38,\n",
       "         'Hyung-taik': 1,\n",
       "         'Rafik': 64,\n",
       "         'Hariri': 106,\n",
       "         'Hillary': 32,\n",
       "         'Rodham': 2,\n",
       "         'Mrs.': 36,\n",
       "         'San': 30,\n",
       "         'Suu': 39,\n",
       "         'Kyi': 41,\n",
       "         'Vijay': 1,\n",
       "         'Nambiar': 2,\n",
       "         'U.N.': 5,\n",
       "         'Ban': 15,\n",
       "         'Ki-moon': 9,\n",
       "         'Muhsin': 1,\n",
       "         'Musa': 4,\n",
       "         'Matwali': 1,\n",
       "         'Atwah': 1,\n",
       "         'Port': 2,\n",
       "         'XVI': 20,\n",
       "         'David': 78,\n",
       "         'Barno': 5,\n",
       "         'Helmut': 1,\n",
       "         'Kohl': 2,\n",
       "         'Herbert': 3,\n",
       "         'Walker': 4,\n",
       "         'Gorbachev': 2,\n",
       "         'Chancellor': 45,\n",
       "         'Geir': 2,\n",
       "         'Haarde': 1,\n",
       "         'Zurab': 5,\n",
       "         'Zhvania': 7,\n",
       "         'National': 2,\n",
       "         'Security': 2,\n",
       "         'Advisor': 1,\n",
       "         'Junichiro': 33,\n",
       "         'Koizumi': 55,\n",
       "         'Toshiyuki': 1,\n",
       "         'Takano': 1,\n",
       "         'Raul': 46,\n",
       "         'Baduel': 2,\n",
       "         'Gholamreza': 3,\n",
       "         'Aghazadeh': 4,\n",
       "         'Fyodor': 1,\n",
       "         'Yurchikhin': 1,\n",
       "         'Oleg': 2,\n",
       "         'Kotov': 2,\n",
       "         'Charles': 41,\n",
       "         'Simonyi': 7,\n",
       "         'Yuri': 12,\n",
       "         'Gagarin': 1,\n",
       "         'Tyurin': 3,\n",
       "         'Miguel': 12,\n",
       "         'Lopez-Alegria': 3,\n",
       "         'Sunita': 2,\n",
       "         'Williams': 24,\n",
       "         'Jose': 82,\n",
       "         'Rodriguez': 45,\n",
       "         'Zapatero': 21,\n",
       "         'Daltrey': 2,\n",
       "         'Pete': 2,\n",
       "         'Townshend': 2,\n",
       "         'Sri': 79,\n",
       "         'European': 1,\n",
       "         'Union': 1,\n",
       "         'Commissioner': 4,\n",
       "         'Peter': 65,\n",
       "         'Mandelson': 6,\n",
       "         'Lankan': 23,\n",
       "         'Ayman': 32,\n",
       "         'al-Zawahiri': 21,\n",
       "         'Zawahiri': 11,\n",
       "         'Mohamed': 66,\n",
       "         'Amiin': 2,\n",
       "         'Adawe': 1,\n",
       "         'Saed': 1,\n",
       "         'Specialist': 2,\n",
       "         'Graner': 2,\n",
       "         'Sergeant': 8,\n",
       "         'Javal': 1,\n",
       "         'Davis': 20,\n",
       "         'Sabrina': 2,\n",
       "         'Harman': 1,\n",
       "         'Andres': 10,\n",
       "         'Valencia': 8,\n",
       "         'Francisco': 2,\n",
       "         'Galan': 1,\n",
       "         'Richard': 52,\n",
       "         'Boucher': 12,\n",
       "         'al-Majid': 4,\n",
       "         'Chemical': 1,\n",
       "         'Deif': 2,\n",
       "         'II': 34,\n",
       "         'Shaukat': 17,\n",
       "         'Reza': 29,\n",
       "         'Asefi': 19,\n",
       "         'ElBaradei': 39,\n",
       "         'al-Zawahri': 7,\n",
       "         'Khodorkovsky': 54,\n",
       "         'Kurds': 4,\n",
       "         'Sixto': 3,\n",
       "         'Cindy': 6,\n",
       "         'Sheehan': 10,\n",
       "         'Larijani': 31,\n",
       "         'Straw': 35,\n",
       "         'of': 14,\n",
       "         'RAVALOMANANA': 5,\n",
       "         'Jupiter': 8,\n",
       "         'Alaina': 1,\n",
       "         'Alexander': 46,\n",
       "         'Nick': 5,\n",
       "         'Pedro': 5,\n",
       "         'Simon': 14,\n",
       "         'Cowell': 6,\n",
       "         'Leonid': 15,\n",
       "         'Kuchma': 28,\n",
       "         'Georgy': 4,\n",
       "         'Gongadze': 11,\n",
       "         'Kravchenko': 1,\n",
       "         'Laura': 11,\n",
       "         'president': 29,\n",
       "         'Jimmy': 10,\n",
       "         'Carter': 26,\n",
       "         'Sarah': 9,\n",
       "         'Morgan': 7,\n",
       "         'Tsvangirai': 6,\n",
       "         'Fuad': 9,\n",
       "         'Masoum': 2,\n",
       "         'Guenther': 1,\n",
       "         'Beckstein': 1,\n",
       "         'Islamic': 15,\n",
       "         'Jihad': 21,\n",
       "         'Taylor': 30,\n",
       "         'Crane': 2,\n",
       "         'Lansana': 4,\n",
       "         'Conte': 2,\n",
       "         'Sierra': 9,\n",
       "         'Leone': 9,\n",
       "         'Mulino': 2,\n",
       "         'Rio': 1,\n",
       "         'de': 53,\n",
       "         'Janeiro': 1,\n",
       "         'Carolyn': 13,\n",
       "         'Presutti': 9,\n",
       "         'Kofi': 121,\n",
       "         'Annan': 192,\n",
       "         'Laurent': 23,\n",
       "         'Gbagbo': 30,\n",
       "         'Fitzgerald': 3,\n",
       "         'Karl': 10,\n",
       "         'Rove': 6,\n",
       "         'Lewis': 9,\n",
       "         'Libby': 17,\n",
       "         'Olli': 5,\n",
       "         'Heinonen': 4,\n",
       "         'Girija': 5,\n",
       "         'Prasad': 9,\n",
       "         'Koirala': 6,\n",
       "         'Krishna': 3,\n",
       "         'Mahara': 1,\n",
       "         'Sitaula': 1,\n",
       "         'McCain': 33,\n",
       "         'Michael': 93,\n",
       "         'Jackson': 53,\n",
       "         'Jalil': 2,\n",
       "         'Jilani': 1,\n",
       "         'Sergei': 53,\n",
       "         'Lavrov': 34,\n",
       "         'Ambassador': 3,\n",
       "         'Woo-ik': 1,\n",
       "         'Christopher': 30,\n",
       "         'Hill': 26,\n",
       "         'Ryan': 11,\n",
       "         'Crocker': 8,\n",
       "         \"Ya'akov\": 1,\n",
       "         'Alperon': 2,\n",
       "         'Dror': 1,\n",
       "         'Yoram': 1,\n",
       "         'Haham': 1,\n",
       "         'U.S.': 8,\n",
       "         'Representative': 2,\n",
       "         'Susan': 5,\n",
       "         'Schwab': 3,\n",
       "         'Alan': 20,\n",
       "         'Garcia': 18,\n",
       "         'Allah': 2,\n",
       "         'Gadahn': 4,\n",
       "         'Melissa': 2,\n",
       "         'Fleming': 3,\n",
       "         'Terje': 8,\n",
       "         'Roed-Larsen': 12,\n",
       "         'Bashar': 45,\n",
       "         'al-Assad': 43,\n",
       "         'Angel': 7,\n",
       "         'Mejia': 2,\n",
       "         'Brad': 13,\n",
       "         'Delp': 10,\n",
       "         'Pamela': 3,\n",
       "         'Sullivan': 2,\n",
       "         'Garang': 30,\n",
       "         'Rick': 5,\n",
       "         'Perry': 11,\n",
       "         'Howard': 40,\n",
       "         'Rudd': 6,\n",
       "         'Nicolas': 32,\n",
       "         'Sarkozy': 55,\n",
       "         'Segolene': 3,\n",
       "         'Royal': 3,\n",
       "         'Jacques': 56,\n",
       "         'Chirac': 74,\n",
       "         'Kyaw': 3,\n",
       "         'Hsann': 2,\n",
       "         'Delphi': 5,\n",
       "         'Corporation': 4,\n",
       "         'Miller': 21,\n",
       "         'Army': 2,\n",
       "         'Private': 1,\n",
       "         'Lynndie': 4,\n",
       "         'England': 13,\n",
       "         'Gyanendra': 18,\n",
       "         'Colonel': 10,\n",
       "         'Barry': 13,\n",
       "         'Johnson': 23,\n",
       "         'Nabil': 5,\n",
       "         'Amr': 9,\n",
       "         'Rupiah': 3,\n",
       "         'Banda': 5,\n",
       "         'Anibal': 1,\n",
       "         'Cavaco': 3,\n",
       "         'H.W.': 5,\n",
       "         'prime': 97,\n",
       "         'Seymour': 2,\n",
       "         'Hersh': 7,\n",
       "         'Roman': 32,\n",
       "         'Catholic': 28,\n",
       "         'Authority': 1,\n",
       "         'Zarqawi': 7,\n",
       "         'Youssef': 11,\n",
       "         'Badran': 1,\n",
       "         'Hoshyar': 11,\n",
       "         'Zebari': 19,\n",
       "         'Karen': 13,\n",
       "         'Kari': 2,\n",
       "         'Barber': 2,\n",
       "         'House': 3,\n",
       "         'Speaker': 5,\n",
       "         'Nancy': 6,\n",
       "         'Pelosi': 8,\n",
       "         'Eric': 7,\n",
       "         'Holder': 4,\n",
       "         'Boris': 12,\n",
       "         'Tarasyuk': 5,\n",
       "         'Bremer': 5,\n",
       "         'al-Aqili': 1,\n",
       "         'Aqili': 1,\n",
       "         'Jarome': 1,\n",
       "         'Iginla': 1,\n",
       "         'Dany': 1,\n",
       "         'Heatley': 1,\n",
       "         'Shane': 5,\n",
       "         'Doan': 1,\n",
       "         'St.': 3,\n",
       "         'Louis': 6,\n",
       "         'Richards': 10,\n",
       "         'Daniel': 28,\n",
       "         'Tjarnqvist': 1,\n",
       "         'Henrik': 1,\n",
       "         'Sedin': 1,\n",
       "         'Ayatollah': 22,\n",
       "         'Khamenei': 16,\n",
       "         'Kabul': 26,\n",
       "         'Sir': 2,\n",
       "         'Allan': 1,\n",
       "         'KEMAKEZA': 1,\n",
       "         'Al': 23,\n",
       "         'Gore': 13,\n",
       "         'Rosales': 3,\n",
       "         'Ilyas': 1,\n",
       "         'Shurpayev': 1,\n",
       "         'Tehran': 15,\n",
       "         'Benon': 3,\n",
       "         'Sevan': 8,\n",
       "         'Federal': 1,\n",
       "         'Reserve': 1,\n",
       "         'Rogers': 5,\n",
       "         'Lerner': 1,\n",
       "         'Mento': 1,\n",
       "         'Tshabalala-Msimang': 1,\n",
       "         'Hayabullah': 1,\n",
       "         'Rafiqi': 1,\n",
       "         'Hu': 101,\n",
       "         'Jintao': 51,\n",
       "         'Yousuf': 20,\n",
       "         'Stanizai': 2,\n",
       "         'Najibullah': 2,\n",
       "         'Sebastien': 3,\n",
       "         'Loeb': 10,\n",
       "         'Petter': 3,\n",
       "         'Solberg': 3,\n",
       "         'Marcus': 3,\n",
       "         'Gronholm': 2,\n",
       "         'Gigi': 2,\n",
       "         'Galli': 2,\n",
       "         'Markko': 1,\n",
       "         'Antonio': 22,\n",
       "         'Alonso': 11,\n",
       "         'Philippe': 5,\n",
       "         'Douste-Blazy': 5,\n",
       "         'Oaxaca': 1,\n",
       "         'Governor': 7,\n",
       "         'Ulises': 2,\n",
       "         'Ruiz': 3,\n",
       "         'Haider': 7,\n",
       "         'Denis': 6,\n",
       "         'Sassou-Nguesso': 2,\n",
       "         'Konan': 4,\n",
       "         'Banny': 9,\n",
       "         'Hosni': 62,\n",
       "         'Mubarak': 120,\n",
       "         'Yulia': 16,\n",
       "         'Tymoshenko': 34,\n",
       "         'Viktor': 125,\n",
       "         'Pinchuk': 1,\n",
       "         'Manmohan': 50,\n",
       "         'Singh': 103,\n",
       "         'Kalam': 4,\n",
       "         'Siad': 18,\n",
       "         'Barre': 21,\n",
       "         'Seyoum': 2,\n",
       "         'Mesfin': 3,\n",
       "         'mayor': 3,\n",
       "         'Dheere': 3,\n",
       "         'Interim': 1,\n",
       "         'Gedi': 17,\n",
       "         'Zuloaga': 4,\n",
       "         'Kate': 4,\n",
       "         'Peyton': 4,\n",
       "         'Madhuri': 1,\n",
       "         'Gupta': 2,\n",
       "         'Lion': 8,\n",
       "         '\"': 27,\n",
       "         'Mil': 19,\n",
       "         'Arcega': 19,\n",
       "         'Energy': 2,\n",
       "         'Ivan': 13,\n",
       "         'Plachkov': 7,\n",
       "         'Khristenko': 2,\n",
       "         'Yuriy': 5,\n",
       "         'Judge': 11,\n",
       "         'Anup': 1,\n",
       "         'Raj': 1,\n",
       "         'Sharma': 4,\n",
       "         'Jawid': 1,\n",
       "         'Imad': 1,\n",
       "         'Moustapha': 2,\n",
       "         'Rene': 23,\n",
       "         'Preval': 53,\n",
       "         'Leslie': 6,\n",
       "         'Manigat': 4,\n",
       "         'Baker': 6,\n",
       "         'Zelaya': 12,\n",
       "         'Ricardo': 17,\n",
       "         'Martinez': 3,\n",
       "         'Roberto': 7,\n",
       "         'Micheletti': 4,\n",
       "         'Hiroyuki': 4,\n",
       "         'Hosoda': 3,\n",
       "         'Office': 1,\n",
       "         'Kim': 70,\n",
       "         'Howells': 1,\n",
       "         'Joao': 2,\n",
       "         'Bernardo': 3,\n",
       "         'Vieira': 3,\n",
       "         'Sanha': 1,\n",
       "         'Viera': 2,\n",
       "         'Chung': 16,\n",
       "         'Dong-young': 5,\n",
       "         'Henk': 1,\n",
       "         'Morsink': 2,\n",
       "         'Armitage': 10,\n",
       "         'Gul': 21,\n",
       "         'Damascus': 9,\n",
       "         'Dhia': 1,\n",
       "         'Najim': 3,\n",
       "         'McClellan': 31,\n",
       "         'al-Shahristani': 3,\n",
       "         'al-Jaafari': 39,\n",
       "         'Adel': 11,\n",
       "         'Abdul-Mahdi': 5,\n",
       "         'Jill': 18,\n",
       "         'Carroll': 30,\n",
       "         'Rangel': 19,\n",
       "         'Michele': 5,\n",
       "         'Alliot-Marie': 6,\n",
       "         'Jan': 26,\n",
       "         'Balkenende': 5,\n",
       "         'Anderson': 15,\n",
       "         'Ray': 15,\n",
       "         'Jennifer': 9,\n",
       "         'Lopez': 25,\n",
       "         'Andrei': 6,\n",
       "         'Melnichenko': 2,\n",
       "         'Duelfer': 5,\n",
       "         'Jaap': 7,\n",
       "         'Hoop': 9,\n",
       "         'Scheffer': 10,\n",
       "         'Winter': 1,\n",
       "         'Beatrice': 2,\n",
       "         'Mtetwa': 3,\n",
       "         'Toby': 3,\n",
       "         'Harnden': 3,\n",
       "         'Julian': 3,\n",
       "         'Simmonds': 3,\n",
       "         'Rock': 9,\n",
       "         'Roll': 4,\n",
       "         'Hall': 20,\n",
       "         'Black': 2,\n",
       "         'Sabbath': 1,\n",
       "         'Lynyrd': 1,\n",
       "         'Skynyrd': 1,\n",
       "         'Miles': 2,\n",
       "         'Herb': 1,\n",
       "         'Alpert': 1,\n",
       "         'Jerry': 6,\n",
       "         'Moss': 1,\n",
       "         'Asher': 2,\n",
       "         'Weisgan': 1,\n",
       "         'Franz': 3,\n",
       "         'Muentefering': 4,\n",
       "         'Andrea': 2,\n",
       "         'Nahles': 1,\n",
       "         'Kajo': 1,\n",
       "         'Wasserhoevel': 1,\n",
       "         'Angela': 41,\n",
       "         'Rosneft': 8,\n",
       "         'Alexei': 5,\n",
       "         'Bogdanchikov': 2,\n",
       "         'Weah': 16,\n",
       "         'Frances': 5,\n",
       "         'Morris': 7,\n",
       "         'Booth': 1,\n",
       "         'Staff': 1,\n",
       "         'Rodrigo': 7,\n",
       "         'Galam': 1,\n",
       "         'Rep.': 1,\n",
       "         'AUSTIN': 1,\n",
       "         'ZALKIN': 1,\n",
       "         'Gen.': 7,\n",
       "         'NE': 2,\n",
       "         'WIN': 2,\n",
       "         'Public': 1,\n",
       "         'debt-to-GDP': 1,\n",
       "         'Kurmanbek': 12,\n",
       "         'Bakiyev': 16,\n",
       "         'Gates': 52,\n",
       "         'Sean': 28,\n",
       "         'McCormack': 20,\n",
       "         'Warsaw': 4,\n",
       "         'Jerzy': 4,\n",
       "         'Szmajdzinski': 4,\n",
       "         'Ayad': 7,\n",
       "         'Gerhard': 27,\n",
       "         'Schroeder': 51,\n",
       "         'Farris': 1,\n",
       "         'Shaharyar': 1,\n",
       "         'Salman': 8,\n",
       "         'Butt': 4,\n",
       "         'Tulsi': 1,\n",
       "         'Giri': 1,\n",
       "         'Medvedev': 46,\n",
       "         'Gotovina': 5,\n",
       "         'Cermak': 3,\n",
       "         'Mladen': 3,\n",
       "         'Markac': 3,\n",
       "         '-': 39,\n",
       "         'Western': 59,\n",
       "         'Muslim': 54,\n",
       "         'Ursula': 4,\n",
       "         'Plassnik': 2,\n",
       "         'Khatami': 44,\n",
       "         'Shirin': 4,\n",
       "         'Ebadi': 7,\n",
       "         'Luther': 6,\n",
       "         'Olusegun': 14,\n",
       "         'Obasanjo': 25,\n",
       "         'Abuja': 3,\n",
       "         'Thabo': 14,\n",
       "         'Mbeki': 16,\n",
       "         'Jacob': 9,\n",
       "         'Zuma': 28,\n",
       "         'Merkel': 66,\n",
       "         'El': 19,\n",
       "         'Salvador': 24,\n",
       "         'Costa': 11,\n",
       "         'Rica': 8,\n",
       "         'Zardari': 24,\n",
       "         'Cameron': 7,\n",
       "         'Le': 6,\n",
       "         'Monde': 2,\n",
       "         'Bruce': 4,\n",
       "         'Arena': 2,\n",
       "         'Askar': 14,\n",
       "         'Akayev': 24,\n",
       "         'Connie': 2,\n",
       "         'Mack': 3,\n",
       "         'Yushchenko': 124,\n",
       "         'Justine': 4,\n",
       "         'Henin-Hardenne': 7,\n",
       "         'Francesca': 1,\n",
       "         'Schiavone': 1,\n",
       "         'Blake': 8,\n",
       "         'Nikolay': 2,\n",
       "         'Davydenko': 2,\n",
       "         'Andreas': 5,\n",
       "         'Seppi': 2,\n",
       "         'Mayor': 30,\n",
       "         'Bloomberg': 7,\n",
       "         'Toussaint': 3,\n",
       "         'Foundation': 7,\n",
       "         'H.I.V./AIDS': 1,\n",
       "         'Initiative': 1,\n",
       "         'Muse': 1,\n",
       "         'Hizbul': 5,\n",
       "         'Islam': 28,\n",
       "         'Phan': 1,\n",
       "         'Van': 12,\n",
       "         'Khai': 1,\n",
       "         'Tran': 1,\n",
       "         'Duc': 1,\n",
       "         'Luong': 1,\n",
       "         'Kandani': 2,\n",
       "         'Ngwira': 5,\n",
       "         'Willy': 1,\n",
       "         'Mwaluka': 1,\n",
       "         'Valery': 2,\n",
       "         'Sitnikov': 1,\n",
       "         'Stanley': 3,\n",
       "         'Tookie': 1,\n",
       "         'Yazdi': 1,\n",
       "         'Mohammad': 75,\n",
       "         'Bager': 1,\n",
       "         'Qalibaf': 2,\n",
       "         'Nasrallah': 11,\n",
       "         'Melinda': 17,\n",
       "         'Smith': 40,\n",
       "         'Hooper': 3,\n",
       "         'Desi': 1,\n",
       "         'Bouterse': 3,\n",
       "         'Netanyahu': 44,\n",
       "         'Jean': 18,\n",
       "         'Bertrand': 3,\n",
       "         'Aristide': 69,\n",
       "         'Yvon': 1,\n",
       "         'Neptune': 1,\n",
       "         'Jocelerme': 1,\n",
       "         'Privert': 1,\n",
       "         'Saad': 14,\n",
       "         'Rafiq': 4,\n",
       "         'Recep': 34,\n",
       "         'Tayyip': 40,\n",
       "         'Erdogan': 65,\n",
       "         'District': 1,\n",
       "         'Federico': 4,\n",
       "         'Moreno': 5,\n",
       "         'Sheikh': 33,\n",
       "         'Khalaf': 6,\n",
       "         'al-Ilayan': 1,\n",
       "         'Anil': 3,\n",
       "         'Kumble': 3,\n",
       "         'Mengistu': 1,\n",
       "         'Haile': 2,\n",
       "         'Mariam': 2,\n",
       "         'Abdi': 3,\n",
       "         'Sibghatullah': 1,\n",
       "         'Mujaddedi': 2,\n",
       "         'Qari': 2,\n",
       "         'Yousaf': 2,\n",
       "         'Mullah': 30,\n",
       "         'Shmatko': 2,\n",
       "         'Masud': 1,\n",
       "         'Mir-Kazemi': 1,\n",
       "         'Fargo': 2,\n",
       "         'Mercy': 1,\n",
       "         'Shimon': 17,\n",
       "         'Peres': 33,\n",
       "         'Mohmmed': 1,\n",
       "         'Dahlan': 7,\n",
       "         'Thad': 4,\n",
       "         'Allen': 11,\n",
       "         'Aktham': 2,\n",
       "         'Naisse': 4,\n",
       "         'Sunni': 114,\n",
       "         'Badr': 1,\n",
       "         'Rashid': 23,\n",
       "         'Waheed': 5,\n",
       "         'Arshad': 4,\n",
       "         'Kye-gwan': 1,\n",
       "         'COLUMBUS': 9,\n",
       "         'CASTRO': 3,\n",
       "         'KIM': 6,\n",
       "         'Il': 8,\n",
       "         'Sung': 5,\n",
       "         'Pyongyang': 13,\n",
       "         'Jong': 11,\n",
       "         'Savo': 1,\n",
       "         'Todovic': 3,\n",
       "         'Queen': 9,\n",
       "         'Sofia': 2,\n",
       "         'Idriss': 13,\n",
       "         'Deby': 21,\n",
       "         'Mohamuud': 1,\n",
       "         'Musse': 1,\n",
       "         'Gurage': 1,\n",
       "         'Brigadier-General': 1,\n",
       "         'Don': 4,\n",
       "         'Tareq': 4,\n",
       "         'Jews': 9,\n",
       "         'Jama': 1,\n",
       "         'Masjid': 1,\n",
       "         'Eid': 7,\n",
       "         'Romania': 3,\n",
       "         'Ivaylo': 1,\n",
       "         'Kalfin': 1,\n",
       "         'Ben': 18,\n",
       "         'Hamed': 3,\n",
       "         'Aberrahman': 1,\n",
       "         'Fred': 5,\n",
       "         'Eckhard': 3,\n",
       "         'Cavic': 1,\n",
       "         'Raouf': 2,\n",
       "         'Abdel': 22,\n",
       "         'Rahman': 14,\n",
       "         'Tomas': 10,\n",
       "         'Ojea': 3,\n",
       "         'Quintana': 5,\n",
       "         'Hilda': 3,\n",
       "         'Molina': 8,\n",
       "         'Nestor': 8,\n",
       "         'Kirchner': 10,\n",
       "         'Vasilily': 1,\n",
       "         'Filipchuk': 1,\n",
       "         'Manouchehr': 19,\n",
       "         'Mottaki': 24,\n",
       "         'Prince': 24,\n",
       "         'al-Faisal': 5,\n",
       "         'Egeland': 14,\n",
       "         'Baloyi': 2,\n",
       "         'al-Bashir': 23,\n",
       "         'Ma': 10,\n",
       "         'Ying-jeou': 2,\n",
       "         ...})"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_dict['per']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_sentance = pos_tag(word_tokenize(sentance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    " \n",
    "def features(tokens, index, history):\n",
    "    \"\"\"\n",
    "    `tokens`  = a POS-tagged sentence [(w1, t1), ...]\n",
    "    `index`   = the index of the token we want to extract features for\n",
    "    `history` = the previous predicted IOB tags\n",
    "    \"\"\"\n",
    " \n",
    "    # init the stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    " \n",
    "    # Pad the sequence with placeholders\n",
    "    tokens = [('[START2]', '[START2]'), ('[START1]', '[START1]')] + list(tokens) + [('[END1]', '[END1]'), ('[END2]', '[END2]')]\n",
    "    history = ['[START2]', '[START1]'] + list(history)\n",
    " \n",
    "    # shift the index with 2, to accommodate the padding\n",
    "    index += 2\n",
    " \n",
    "    word, pos = tokens[index]\n",
    "    prevword, prevpos = tokens[index - 1]\n",
    "    prevprevword, prevprevpos = tokens[index - 2]\n",
    "    nextword, nextpos = tokens[index + 1]\n",
    "    nextnextword, nextnextpos = tokens[index + 2]\n",
    "    previob = history[index - 1]\n",
    "    contains_dash = '-' in word\n",
    "    contains_dot = '.' in word\n",
    "    allascii = all([True for c in word if c in string.ascii_lowercase])\n",
    " \n",
    "    allcaps = word == word.capitalize()\n",
    "    capitalized = word[0] in string.ascii_uppercase\n",
    " \n",
    "    prevallcaps = prevword == prevword.capitalize()\n",
    "    prevcapitalized = prevword[0] in string.ascii_uppercase\n",
    " \n",
    "    nextallcaps = prevword == prevword.capitalize()\n",
    "    nextcapitalized = prevword[0] in string.ascii_uppercase\n",
    " \n",
    "    return {\n",
    "        'word': word,\n",
    "        'lemma': stemmer.stem(word),\n",
    "        'pos': pos,\n",
    "        'all-ascii': allascii,\n",
    " \n",
    "        'next-word': nextword,\n",
    "        'next-lemma': stemmer.stem(nextword),\n",
    "        'next-pos': nextpos,\n",
    " \n",
    "        'next-next-word': nextnextword,\n",
    "        'nextnextpos': nextnextpos,\n",
    " \n",
    "        'prev-word': prevword,\n",
    "        'prev-lemma': stemmer.stem(prevword),\n",
    "        'prev-pos': prevpos,\n",
    " \n",
    "        'prev-prev-word': prevprevword,\n",
    "        'prev-prev-pos': prevprevpos,\n",
    " \n",
    "        'prev-iob': previob,\n",
    " \n",
    "        'contains-dash': contains_dash,\n",
    "        'contains-dot': contains_dot,\n",
    " \n",
    "        'all-caps': allcaps,\n",
    "        'capitalized': capitalized,\n",
    " \n",
    "        'prev-all-caps': prevallcaps,\n",
    "        'prev-capitalized': prevcapitalized,\n",
    " \n",
    "        'next-all-caps': nextallcaps,\n",
    "        'next-capitalized': nextcapitalized,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_conll_iob(annotated_sentence):\n",
    "    \"\"\"\n",
    "    `annotated_sentence` = list of triplets [(w1, t1, iob1), ...]\n",
    "    Transform a pseudo-IOB notation: O, PERSON, PERSON, O, O, LOCATION, O\n",
    "    to proper IOB notation: O, B-PERSON, I-PERSON, O, O, B-LOCATION, O\n",
    "    \"\"\"\n",
    "    proper_iob_tokens = []\n",
    "    for idx, annotated_token in enumerate(annotated_sentence):\n",
    "        tag, word, ner = annotated_token\n",
    " \n",
    "        if ner != 'O':\n",
    "            if idx == 0:\n",
    "                ner = \"B-\" + ner\n",
    "            elif annotated_sentence[idx - 1][2] == ner:\n",
    "                ner = \"I-\" + ner\n",
    "            else:\n",
    "                ner = \"B-\" + ner\n",
    "        proper_iob_tokens.append((tag, word, ner))\n",
    "    return proper_iob_tokens\n",
    " \n",
    "def read_gmb(corpus_root):\n",
    "    for root, dirs, files in os.walk(corpus_root):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".tags\"):\n",
    "                with open(os.path.join(root, filename), 'rb') as file_handle:\n",
    "                    file_content = file_handle.read().decode('utf-8').strip()\n",
    "                    annotated_sentences = file_content.split('\\n\\n')\n",
    "                    for annotated_sentence in annotated_sentences:\n",
    "                        annotated_tokens = [seq for seq in annotated_sentence.split('\\n') if seq]\n",
    " \n",
    "                        standard_form_tokens = []\n",
    " \n",
    "                        for idx, annotated_token in enumerate(annotated_tokens):\n",
    "                            annotations = annotated_token.split('\\t')\n",
    "                            word, tag, ner = annotations[0], annotations[1], annotations[3]\n",
    " \n",
    "                            if ner != 'O':\n",
    "                                ner = ner.split('-')[0]\n",
    " \n",
    "                            if tag in ('LQU', 'RQU'):   # Make it NLTK compatible\n",
    "                                tag = \"``\"\n",
    " \n",
    "                            standard_form_tokens.append((word, tag, ner))\n",
    " \n",
    "                        conll_tokens = to_conll_iob(standard_form_tokens)\n",
    " \n",
    "                        # Make it NLTK Classifier compatible - [(w1, t1, iob1), ...] to [((w1, t1), iob1), ...]\n",
    "                        # Because the classfier expects a tuple as input, first item input, second the class\n",
    "                        yield [((w, t), iob) for w, t, iob in conll_tokens]\n",
    "\n",
    "reader = read_gmb(corpus_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Thousands', 'NNS'), 'O'),\n",
       " (('of', 'IN'), 'O'),\n",
       " (('demonstrators', 'NNS'), 'O'),\n",
       " (('have', 'VBP'), 'O'),\n",
       " (('marched', 'VBN'), 'O'),\n",
       " (('through', 'IN'), 'O'),\n",
       " (('London', 'NNP'), 'B-geo'),\n",
       " (('to', 'TO'), 'O'),\n",
       " (('protest', 'VB'), 'O'),\n",
       " (('the', 'DT'), 'O'),\n",
       " (('war', 'NN'), 'O'),\n",
       " (('in', 'IN'), 'O'),\n",
       " (('Iraq', 'NNP'), 'B-geo'),\n",
       " (('and', 'CC'), 'O'),\n",
       " (('demand', 'VB'), 'O'),\n",
       " (('the', 'DT'), 'O'),\n",
       " (('withdrawal', 'NN'), 'O'),\n",
       " (('of', 'IN'), 'O'),\n",
       " (('British', 'JJ'), 'B-gpe'),\n",
       " (('troops', 'NNS'), 'O'),\n",
       " (('from', 'IN'), 'O'),\n",
       " (('that', 'DT'), 'O'),\n",
       " (('country', 'NN'), 'O'),\n",
       " (('.', '.'), 'O')]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createGenerator():\n",
    "    mylist = range(3)\n",
    "    for i in mylist:\n",
    "        yield i*i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = createGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Iterable\n",
    "from nltk.tag import ClassifierBasedTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    " \n",
    "class NamedEntityChunker(ChunkParserI):\n",
    "    def __init__(self, train_sents, **kwargs):\n",
    "        assert isinstance(train_sents, Iterable)\n",
    " \n",
    "        self.feature_detector = features\n",
    "        self.tagger = ClassifierBasedTagger(\n",
    "            train=train_sents,\n",
    "            feature_detector=features,\n",
    "            **kwargs)\n",
    " \n",
    "    def parse(self, tagged_sent):\n",
    "        chunks = self.tagger.tag(tagged_sent)\n",
    " \n",
    "        # Transform the result from [((w1, t1), iob1), ...] \n",
    "        # to the preferred list of triplets format [(w1, t1, iob1), ...]\n",
    "        iob_triplets = [(w, t, c) for ((w, t), c) in chunks]\n",
    " \n",
    "        # Transform the list of triplets to nltk.Tree format\n",
    "        return conlltags2tree(iob_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#training samples = 55809\n",
      "#test samples = 6201\n"
     ]
    }
   ],
   "source": [
    "reader = read_gmb(corpus_root)\n",
    "data = list(reader)\n",
    "training_samples = data[:int(len(data) * 0.9)]\n",
    "test_samples = data[int(len(data) * 0.9):]\n",
    " \n",
    "print(\"#training samples = %s\" % len(training_samples))    # training samples = 55809\n",
    "print(\"#test samples = %s\" % len(test_samples))                # test samples = 6201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunker = NamedEntityChunker(training_samples[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (gpe Machine/NN)\n",
      "  learning/NN\n",
      "  is/VBZ\n",
      "  the/DT\n",
      "  science/NN\n",
      "  of/IN\n",
      "  getting/VBG\n",
      "  computers/NNS\n",
      "  to/TO\n",
      "  act/VB\n",
      "  without/IN\n",
      "  being/VBG\n",
      "  explicitly/RB\n",
      "  programmed/VBN\n",
      "  ./.\n",
      "  In/IN\n",
      "  the/DT\n",
      "  past/JJ\n",
      "  decade/NN\n",
      "  ,/,\n",
      "  machine/NN\n",
      "  learning/NN\n",
      "  has/VBZ\n",
      "  given/VBN\n",
      "  us/PRP\n",
      "  self-driving/JJ\n",
      "  (tim cars/NNS)\n",
      "  ,/,\n",
      "  practical/JJ\n",
      "  speech/NN\n",
      "  recognition/NN\n",
      "  ,/,\n",
      "  effective/JJ\n",
      "  web/NN\n",
      "  search/NN\n",
      "  ,/,\n",
      "  and/CC\n",
      "  a/DT\n",
      "  vastly/RB\n",
      "  improved/VBN\n",
      "  understanding/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  human/JJ\n",
      "  genome/NN\n",
      "  ./.\n",
      "  (per Machine/NNP learning/NN)\n",
      "  is/VBZ\n",
      "  so/RB\n",
      "  pervasive/JJ\n",
      "  (tim today/NN)\n",
      "  that/IN\n",
      "  you/PRP\n",
      "  probably/RB\n",
      "  use/VBP\n",
      "  it/PRP\n",
      "  dozens/VBZ\n",
      "  of/IN\n",
      "  times/NNS\n",
      "  a/DT\n",
      "  day/NN\n",
      "  without/IN\n",
      "  knowing/VBG\n",
      "  it/PRP\n",
      "  ./.\n",
      "  Many/JJ\n",
      "  researchers/NNS\n",
      "  also/RB\n",
      "  think/VBP\n",
      "  it/PRP\n",
      "  is/VBZ\n",
      "  the/DT\n",
      "  best/JJS\n",
      "  way/NN\n",
      "  to/TO\n",
      "  make/VB\n",
      "  progress/NN\n",
      "  towards/IN\n",
      "  (tim human-level/NN AI/NNP)\n",
      "  ./.\n",
      "  In/IN\n",
      "  this/DT\n",
      "  class/NN\n",
      "  ,/,\n",
      "  you/PRP\n",
      "  will/MD\n",
      "  learn/VB\n",
      "  about/IN\n",
      "  the/DT\n",
      "  most/RBS\n",
      "  effective/JJ\n",
      "  machine/NN\n",
      "  learning/VBG\n",
      "  (tim techniques/NNS)\n",
      "  ,/,\n",
      "  and/CC\n",
      "  gain/NN\n",
      "  practice/NN\n",
      "  implementing/VBG\n",
      "  them/PRP\n",
      "  and/CC\n",
      "  getting/VBG\n",
      "  them/PRP\n",
      "  to/TO\n",
      "  work/VB\n",
      "  for/IN\n",
      "  yourself/PRP\n",
      "  ./.\n",
      "  More/RBR\n",
      "  importantly/RB\n",
      "  ,/,\n",
      "  you/PRP\n",
      "  'll/MD\n",
      "  learn/VB\n",
      "  about/IN\n",
      "  not/RB\n",
      "  only/RB\n",
      "  the/DT\n",
      "  (org theoretical/JJ)\n",
      "  underpinnings/NNS\n",
      "  of/IN\n",
      "  learning/NN\n",
      "  ,/,\n",
      "  but/CC\n",
      "  also/RB\n",
      "  gain/VBP\n",
      "  the/DT\n",
      "  practical/JJ\n",
      "  know-how/NN\n",
      "  needed/VBN\n",
      "  to/TO\n",
      "  quickly/RB\n",
      "  and/CC\n",
      "  powerfully/RB\n",
      "  apply/VB\n",
      "  these/DT\n",
      "  techniques/NNS\n",
      "  to/TO\n",
      "  new/JJ\n",
      "  problems/NNS\n",
      "  ./.\n",
      "  Finally/RB\n",
      "  ,/,\n",
      "  you/PRP\n",
      "  'll/MD\n",
      "  learn/VB\n",
      "  about/IN\n",
      "  some/DT\n",
      "  of/IN\n",
      "  (geo Silicon/NNP Valley/NNP)\n",
      "  (org 's/POS)\n",
      "  best/JJS\n",
      "  practices/NNS\n",
      "  in/IN\n",
      "  (tim innovation/NN)\n",
      "  as/IN\n",
      "  it/PRP\n",
      "  pertains/VBZ\n",
      "  to/TO\n",
      "  machine/NN\n",
      "  learning/NN\n",
      "  and/CC\n",
      "  (geo AI/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "print(chunker.parse(pos_sentance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9289662797125484\n"
     ]
    }
   ],
   "source": [
    "score = chunker.evaluate([conlltags2tree([(w, t, iob) for (w, t), iob in iobs]) for iobs in test_samples[:500]])\n",
    "print(score.accuracy())        # 0.931132334092 - Awesome :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Product Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.data.path.append(\"/Volumes/Secondary/\")\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kitch_review = pd.read_json('/Volumes/Secondary/Downloads/amazon/sub_reviews_Home_and_Kitchen.json',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210000, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kitch_review.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core electronics too big for my mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#core_elect = pd.read_json('/Volumes/Secondary/Downloads/amazon/sub_reviews_Electronics_5.json',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#core_elect = pd.read_json('/Volumes/Secondary/Downloads/amazon/reviews_Electronics_5.json',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "core_elect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>found_helpful</th>\n",
       "      <th>outof_helpful</th>\n",
       "      <th>ratio_helpful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0528881469</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>We got this GPS for my husband who is an (OTR)...</td>\n",
       "      <td>06 2, 2013</td>\n",
       "      <td>AO94DHGC771SJ</td>\n",
       "      <td>amazdnu</td>\n",
       "      <td>Gotta have GPS!</td>\n",
       "      <td>1370131200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin helpful  overall  \\\n",
       "0  0528881469  [0, 0]        5   \n",
       "\n",
       "                                          reviewText  reviewTime  \\\n",
       "0  We got this GPS for my husband who is an (OTR)...  06 2, 2013   \n",
       "\n",
       "      reviewerID reviewerName          summary  unixReviewTime  found_helpful  \\\n",
       "0  AO94DHGC771SJ      amazdnu  Gotta have GPS!      1370131200              0   \n",
       "\n",
       "   outof_helpful  ratio_helpful  \n",
       "0              0            NaN  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_elect.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add found_helpful and outof_helpful\n",
    "- Is there a way to differentiate between helpful and nonhelpful reviews based on wording?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def helpful(df):\n",
    "    def split_list_helpful(col):\n",
    "        helpful = col[0]\n",
    "        return helpful\n",
    "    def split_list_outof_helpful(col):\n",
    "        outof_helpful = col[1]\n",
    "        return outof_helpful\n",
    "\n",
    "    df['found_helpful'] = df['helpful'].apply(split_list_helpful)\n",
    "    df['outof_helpful'] = df['helpful'].apply(split_list_outof_helpful)\n",
    "    df['ratio_helpful'] = df['found_helpful']/df['outof_helpful']\n",
    "    df['ratio_greater_'+str(perc)] = df['ratio_helpful'] > perc\n",
    "    df['ratio_less_'+str(perc)] = df['ratio_helpful'] < perc\n",
    "    df.dropna(inplace=True,axis=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kitch_review = helpful(kitch_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108008, 14)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kitch_review.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = kitch_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many more helpful than nonhelpful reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perc = .75\n",
    "over = df[df['ratio_helpful'] > perc]['asin'].count()\n",
    "under = df[df['ratio_helpful'] < perc]['asin'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78278 reviews judged over 50% helpful\n",
      "27338 reviews judged under 50% helpful\n",
      "0.25884335706711104\n"
     ]
    }
   ],
   "source": [
    "print('{} reviews judged over 50% helpful'.format(over))\n",
    "print('{} reviews judged under 50% helpful'.format(under))\n",
    "print('{}'.format(under/(over+under)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce to only reviews that have more than 3 ratings for helpfulness\n",
    "- Many reviews have single judgements, arbitrary but need to have at least a few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[df['outof_helpful'] >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perc = .75\n",
    "over = df[df['ratio_helpful'] > perc]['asin'].count()\n",
    "under = df[df['ratio_helpful'] < perc]['asin'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37557 reviews judged over 50% helpful\n",
      "13741 reviews judged under 50% helpful\n",
      "0.2678661936137861\n"
     ]
    }
   ],
   "source": [
    "print('{} reviews judged over 50% helpful'.format(over))\n",
    "print('{} reviews judged under 50% helpful'.format(under))\n",
    "print('{}'.format(under/(over+under)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there different word usage in the summary of helpful vs nonhelpful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>found_helpful</th>\n",
       "      <th>outof_helpful</th>\n",
       "      <th>ratio_helpful</th>\n",
       "      <th>ratio_greater_0.75</th>\n",
       "      <th>ratio_less_0.75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0307394530</td>\n",
       "      <td>[11, 16]</td>\n",
       "      <td>2</td>\n",
       "      <td>I anxiously waited for the book I had pre orde...</td>\n",
       "      <td>06 30, 2008</td>\n",
       "      <td>A31B4D7URW4DNZ</td>\n",
       "      <td>3Gigi3</td>\n",
       "      <td>Mother of the Bride</td>\n",
       "      <td>1214784000</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin   helpful  overall  \\\n",
       "2  0307394530  [11, 16]        2   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "2  I anxiously waited for the book I had pre orde...  06 30, 2008   \n",
       "\n",
       "       reviewerID reviewerName              summary  unixReviewTime  \\\n",
       "2  A31B4D7URW4DNZ       3Gigi3  Mother of the Bride      1214784000   \n",
       "\n",
       "   found_helpful  outof_helpful  ratio_helpful  ratio_greater_0.75  \\\n",
       "2             11             16         0.6875               False   \n",
       "\n",
       "   ratio_less_0.75  \n",
       "2             True  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sents = list(df['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sents_tokenized = []\n",
    "for sent in sents:\n",
    "    sents_tokenized.append(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "additional = [\"''\",\"'s\",'--','``','','',\"'\",\"n't\",\"...\",\"..\"]\n",
    "stop = stopwords.words('english') + list(string.punctuation) + additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import WordNetLemmatizer as wnl\n",
    "\n",
    "sents_tokenized_clean = []\n",
    "cnt_ratio_greater = {}\n",
    "for ratio in df['ratio_greater_0.75'].unique():\n",
    "    cnt = Counter()\n",
    "    for desc in df[df['ratio_greater_0.75'] == ratio]['summary']:\n",
    "        clean = []\n",
    "        for word in [i for i in word_tokenize(desc.lower()) if i not in stop]:\n",
    "            clean.append(wnl().lemmatize(word))\n",
    "            #cnt[wnl().lemmatize(word)] += 1\n",
    "        sents_tokenized_clean.append(clean)\n",
    "    #cnt_ratio_greater[ratio] = cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-10 17:34:51,074 : INFO : collecting all words and their counts\n",
      "2017-07-10 17:34:51,076 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-07-10 17:34:51,089 : INFO : PROGRESS: at sentence #10000, processed 28745 words, keeping 4233 word types\n",
      "2017-07-10 17:34:51,101 : INFO : PROGRESS: at sentence #20000, processed 58886 words, keeping 4948 word types\n",
      "2017-07-10 17:34:51,117 : INFO : PROGRESS: at sentence #30000, processed 90150 words, keeping 7045 word types\n",
      "2017-07-10 17:34:51,131 : INFO : PROGRESS: at sentence #40000, processed 121621 words, keeping 8012 word types\n",
      "2017-07-10 17:34:51,147 : INFO : PROGRESS: at sentence #50000, processed 152983 words, keeping 8012 word types\n",
      "2017-07-10 17:34:51,159 : INFO : collected 8012 word types from a corpus of 164486 raw words and 53690 sentences\n",
      "2017-07-10 17:34:51,161 : INFO : Loading a fresh vocabulary\n",
      "2017-07-10 17:34:51,177 : INFO : min_count=5 retains 2896 unique words (36% of original 8012, drops 5116)\n",
      "2017-07-10 17:34:51,178 : INFO : min_count=5 leaves 152113 word corpus (92% of original 164486, drops 12373)\n",
      "2017-07-10 17:34:51,203 : INFO : deleting the raw counts dictionary of 8012 items\n",
      "2017-07-10 17:34:51,204 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2017-07-10 17:34:51,206 : INFO : downsampling leaves estimated 127977 word corpus (84.1% of prior 152113)\n",
      "2017-07-10 17:34:51,208 : INFO : estimated required memory for 2896 words and 100 dimensions: 3764800 bytes\n",
      "2017-07-10 17:34:51,228 : INFO : resetting layer weights\n",
      "2017-07-10 17:34:51,314 : INFO : training model with 3 workers on 2896 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-07-10 17:34:52,328 : INFO : PROGRESS: at 4.63% examples, 588587 words/s, in_qsize 6, out_qsize 0\n",
      "2017-07-10 17:34:53,333 : INFO : PROGRESS: at 9.44% examples, 599833 words/s, in_qsize 5, out_qsize 0\n",
      "2017-07-10 17:34:54,344 : INFO : PROGRESS: at 14.11% examples, 597303 words/s, in_qsize 5, out_qsize 0\n",
      "2017-07-10 17:34:55,352 : INFO : PROGRESS: at 18.97% examples, 602254 words/s, in_qsize 6, out_qsize 0\n",
      "2017-07-10 17:34:56,364 : INFO : PROGRESS: at 23.77% examples, 603302 words/s, in_qsize 6, out_qsize 0\n",
      "2017-07-10 17:34:57,369 : INFO : PROGRESS: at 28.64% examples, 605931 words/s, in_qsize 6, out_qsize 0\n",
      "2017-07-10 17:34:58,376 : INFO : PROGRESS: at 33.45% examples, 606616 words/s, in_qsize 5, out_qsize 0\n",
      "2017-07-10 17:34:59,385 : INFO : PROGRESS: at 38.25% examples, 606940 words/s, in_qsize 5, out_qsize 0\n",
      "2017-07-10 17:35:00,385 : INFO : PROGRESS: at 42.98% examples, 606903 words/s, in_qsize 5, out_qsize 0\n",
      "2017-07-10 17:35:01,392 : INFO : PROGRESS: at 47.78% examples, 607191 words/s, in_qsize 4, out_qsize 1\n",
      "2017-07-10 17:35:02,405 : INFO : PROGRESS: at 52.71% examples, 608577 words/s, in_qsize 5, out_qsize 0\n",
      "2017-07-10 17:35:03,409 : INFO : PROGRESS: at 57.46% examples, 608194 words/s, in_qsize 6, out_qsize 0\n",
      "2017-07-10 17:35:04,412 : INFO : PROGRESS: at 62.26% examples, 608565 words/s, in_qsize 5, out_qsize 0\n",
      "2017-07-10 17:35:05,446 : INFO : PROGRESS: at 67.24% examples, 609165 words/s, in_qsize 5, out_qsize 2\n",
      "2017-07-10 17:35:06,449 : INFO : PROGRESS: at 72.10% examples, 609879 words/s, in_qsize 6, out_qsize 0\n",
      "2017-07-10 17:35:07,453 : INFO : PROGRESS: at 76.90% examples, 610041 words/s, in_qsize 5, out_qsize 0\n",
      "2017-07-10 17:35:08,469 : INFO : PROGRESS: at 81.47% examples, 607960 words/s, in_qsize 6, out_qsize 0\n",
      "2017-07-10 17:35:09,499 : INFO : PROGRESS: at 85.72% examples, 603446 words/s, in_qsize 4, out_qsize 1\n",
      "2017-07-10 17:35:10,523 : INFO : PROGRESS: at 89.37% examples, 595581 words/s, in_qsize 5, out_qsize 0\n",
      "2017-07-10 17:35:11,524 : INFO : PROGRESS: at 92.89% examples, 588375 words/s, in_qsize 6, out_qsize 1\n",
      "2017-07-10 17:35:12,547 : INFO : PROGRESS: at 96.96% examples, 584563 words/s, in_qsize 4, out_qsize 1\n",
      "2017-07-10 17:35:13,249 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-07-10 17:35:13,251 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-07-10 17:35:13,259 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-07-10 17:35:13,260 : INFO : training on 16448600 raw words (12797141 effective words) took 21.9s, 583331 effective words/s\n"
     ]
    }
   ],
   "source": [
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " \n",
    "#sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "# train word2vec on the two sentences\n",
    "model = gensim.models.Word2Vec(sents_tokenized_clean, min_count=5, iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('foolproof', 0.35323408246040344)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['love', 'fruit'], negative=['hate'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48854908069802289"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('love','great')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "vect = CountVectorizer(tokenizer=LemmaTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X = df['job_summary']\n",
    "X = df['summary']\n",
    "#y = df['title_num']\n",
    "y = df['ratio_greater_0.75']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69951573849878934"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ratio_greater_0.75'].value_counts()[1] / df['ratio_greater_0.75'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74352765878189608"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(lowercase=True, \n",
    "                             strip_accents='unicode', \n",
    "                             stop_words='english',#stop, \n",
    "                             ngram_range=(1,3), \n",
    "                             #min_df=.2, \n",
    "                             max_df=.7,\n",
    "                             tokenizer=LemmaTokenizer())),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "#     ('lsa', TruncatedSVD(n_components=100, random_state=42)),\n",
    "#     ('norm', Normalizer(copy=False)),\n",
    "#     ('tfidf', TfidfVectorizer(lowercase=True, \n",
    "#                               strip_accents='unicode', \n",
    "#                               #analyzer='word', \n",
    "#                               stop_words='english',\n",
    "#                               sublinear_tf=True)),\n",
    "    #('cls', MultinomialNB()),\n",
    "    #('cls', BernoulliNB()),\n",
    "    ('cls', LogisticRegression()),\n",
    "    #('cls', RandomForestClassifier(n_estimators=250, random_state=42))\n",
    "]) \n",
    "pipeline.fit(X_train, y_train)\n",
    "predicted = pipeline.predict(X_test)\n",
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'feature_importances_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-0ecb349f7397>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cls'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'feature_importances_'"
     ]
    }
   ],
   "source": [
    "pipeline.named_steps['cls'].feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Amazon Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting ',' delimiter: line 1 column 2385 (char 2384)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-e23f7e6715ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Volumes/Secondary/Downloads/amazon/2sub_meta_Home_and_Kitchen.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/xbno/anaconda3/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xbno/anaconda3/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xbno/anaconda3/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 1 column 2385 (char 2384)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data = []\n",
    "with open('/Volumes/Secondary/Downloads/amazon/2sub_meta_Home_and_Kitchen.json') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'asin': '0076144011',\n",
       "  'categories': [['Home & Kitchen']],\n",
       "  'imUrl': 'http://g-ecx.images-amazon.com/images/G/01/x-site/icons/no-img-sm._CB192198896_.gif',\n",
       "  'salesRank': {'Books': 6285595},\n",
       "  'title': 'Ninjas, Piranhas, and Galileo'}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asin\": \"0076144011\", \"salesRank\": {\"Books\": 6285595}, \"imUrl\": \"http://g-ecx.images-amazon.com/images/G/01/x-site/icons/no-img-sm._CB192198896_.gif\", \"categories\": [[\"Home & Kitchen\"]], \"title\": \"Ninjas, Piranhas, and Galileo\"}\r\n"
     ]
    }
   ],
   "source": [
    "! cat '/Volumes/Secondary/Downloads/amazon/1sub_meta_Home_and_Kitchen-Saved.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from json import JSONDecoder\n",
    "from functools import partial\n",
    "\n",
    "fileobj = '/Volumes/Secondary/Downloads/amazon/2sub_meta_Home_and_Kitchen-Saved.json'\n",
    "def json_parse(fileobj, decoder=JSONDecoder(), buffersize=2048):\n",
    "    buffer = ''\n",
    "    for chunk in iter(partial(fileobj.read, buffersize), ''):\n",
    "         buffer += chunk\n",
    "         while buffer:\n",
    "             try:\n",
    "                 result, index = decoder.raw_decode(buffer)\n",
    "                 yield result\n",
    "                 buffer = buffer[index:]\n",
    "             except ValueError:\n",
    "                 # Not enough data to decode, read more\n",
    "                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'buffer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-784249cedf45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbuffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'buffer' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
